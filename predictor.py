#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KB Reflex - íˆ¬ì ì‹¬ë¦¬ ë¶„ì„ ì˜ˆì¸¡ ëª¨ë“ˆ
ì‹¤ì‹œê°„ íˆ¬ì ì‹¬ë¦¬ íŒ¨í„´ ë¶„ë¥˜ë¥¼ ìœ„í•œ BERT ê¸°ë°˜ ì˜ˆì¸¡ ì—”ì§„
"""

import os
import json
import torch
import numpy as np
from typing import Dict, Optional
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F


class SentimentPredictor:
    """
    í›ˆë ¨ëœ BERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ íˆ¬ì ì‹¬ë¦¬ íŒ¨í„´ì„ ì˜ˆì¸¡í•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ì‚¬ìš©ìì˜ íˆ¬ì ë©”ëª¨ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ì‹¬ë¦¬ íŒ¨í„´ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤:
    - ê³µí¬ë§¤ë„, ì¶”ê²©ë§¤ìˆ˜, ê³¼ì‹ , ì†ì‹¤íšŒí”¼ ë“±
    """
    
    def __init__(self, model_path: str):
        """
        ì˜ˆì¸¡ ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_path (str): í›ˆë ¨ëœ ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.model_path = model_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ¤– KB Reflex AI ì—”ì§„ ë¡œë”© ì¤‘... (Device: {self.device})")
        
        # ëª¨ë¸ ì •ë³´ ë¡œë“œ
        self._load_model_info()
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        
        print(f"âœ… AI ì—”ì§„ ë¡œë“œ ì™„ë£Œ (í´ë˜ìŠ¤ ìˆ˜: {len(self.id_to_label)})")
        
        # ê° ê°ì • íŒ¨í„´ì— ëŒ€í•œ ì„¤ëª… ì •ì˜
        self._define_pattern_descriptions()
    
    def _load_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë° ë¼ë²¨ ë§¤í•‘ ë¡œë“œ"""
        model_info_path = os.path.join(self.model_path, 'model_info.json')
        
        if not os.path.exists(model_info_path):
            raise FileNotFoundError(f"ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_info_path}")
        
        with open(model_info_path, 'r', encoding='utf-8') as f:
            model_info = json.load(f)
        
        # ë¬¸ìì—´ í‚¤ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
        self.id_to_label = {int(k): v for k, v in model_info['id_to_label'].items()}
        self.label_to_id = model_info['label_to_id']
        self.num_labels = model_info['num_labels']
    
    def _define_pattern_descriptions(self):
        """ê° íˆ¬ì ì‹¬ë¦¬ íŒ¨í„´ì— ëŒ€í•œ ì„¤ëª… ì •ì˜"""
        self.pattern_descriptions = {
            'ê³µí¬': 'ì‹œì¥ í•˜ë½ì´ë‚˜ ì†ì‹¤ì— ëŒ€í•œ ë‘ë ¤ì›€ìœ¼ë¡œ ì¸í•œ ê¸‰ì‘ìŠ¤ëŸ¬ìš´ ë§¤ë„ í–‰ë™ì…ë‹ˆë‹¤. ê°ì •ì  ê²°ì •ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆì–´ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.',
            'ì¶”ê²©ë§¤ìˆ˜': 'ì£¼ê°€ ìƒìŠ¹ì„ ë³´ê³  ê¸°íšŒë¥¼ ë†“ì¹ ê¹Œ ë‘ë ¤ì›Œ ì„œë‘˜ëŸ¬ ë§¤ìˆ˜í•˜ëŠ” FOMO(Fear of Missing Out) ì‹¬ë¦¬ì…ë‹ˆë‹¤.',
            'ê³¼ì‹ ': 'ê³¼ê±°ì˜ ì„±ê³µ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ê³¼ë„í•œ ìì‹ ê°ì„ ë³´ì´ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤. ìœ„í—˜ ê´€ë¦¬ì— ì†Œí™€í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
            'ì†ì‹¤íšŒí”¼': 'ì†ì‹¤ì„ í™•ì •í•˜ê¸° ì‹«ì–´í•˜ì—¬ ì†ì ˆì„ ì§€ì—°ì‹œí‚¤ê±°ë‚˜ í”¼í•˜ë ¤ëŠ” ì‹¬ë¦¬ì…ë‹ˆë‹¤.',
            'í™•ì¦í¸í–¥': 'ìì‹ ì˜ íŒë‹¨ì„ ë’·ë°›ì¹¨í•˜ëŠ” ì •ë³´ë§Œ ì„ íƒì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ë ¤ëŠ” ê²½í–¥ì…ë‹ˆë‹¤.',
            'êµ°ì¤‘ì‹¬ë¦¬': 'ë‹¤ë¥¸ íˆ¬ììë“¤ì˜ í–‰ë™ì„ ë”°ë¼í•˜ë ¤ëŠ” ê²½í–¥ìœ¼ë¡œ, ë…ë¦½ì  ì‚¬ê³ ê°€ ë¶€ì¡±í•  ë•Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.',
            'ëƒ‰ì •': 'ê°ì •ì— íœ˜ë‘˜ë¦¬ì§€ ì•Šê³  í•©ë¦¬ì ìœ¼ë¡œ íˆ¬ì ê²°ì •ì„ ë‚´ë¦¬ëŠ” ì´ìƒì ì¸ ìƒíƒœì…ë‹ˆë‹¤.'
        }
    
    def _get_pattern_description(self, pattern: str) -> str:
        """íŒ¨í„´ì— ëŒ€í•œ ì„¤ëª… ë°˜í™˜"""
        return self.pattern_descriptions.get(pattern, 'ë¶„ì„ëœ íˆ¬ì ì‹¬ë¦¬ íŒ¨í„´ì…ë‹ˆë‹¤.')
    
    def _get_confidence_level(self, confidence: float) -> str:
        """ì‹ ë¢°ë„ ë ˆë²¨ í…ìŠ¤íŠ¸ ë°˜í™˜"""
        if confidence >= 0.9:
            return "ë§¤ìš° ë†’ìŒ"
        elif confidence >= 0.7:
            return "ë†’ìŒ"
        elif confidence >= 0.5:
            return "ë³´í†µ"
        else:
            return "ë‚®ìŒ"
    
    def predict(self, text: str) -> Dict:
        """
        ì…ë ¥ í…ìŠ¤íŠ¸ì˜ íˆ¬ì ì‹¬ë¦¬ íŒ¨í„´ ì˜ˆì¸¡
        
        Args:
            text (str): ë¶„ì„í•  íˆ¬ì ë©”ëª¨ í…ìŠ¤íŠ¸
            
        Returns:
            Dict: ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
                - pattern (str): ì˜ˆì¸¡ëœ ì‹¬ë¦¬ íŒ¨í„´
                - confidence (float): ì˜ˆì¸¡ ì‹ ë¢°ë„ (0-1)
                - confidence_level (str): ì‹ ë¢°ë„ ë ˆë²¨ ("ë†’ìŒ", "ë³´í†µ", "ë‚®ìŒ")
                - description (str): íŒ¨í„´ì— ëŒ€í•œ ì„¤ëª…
                - all_probabilities (dict): ëª¨ë“  í´ë˜ìŠ¤ë³„ í™•ë¥ 
        """
        if not text or not text.strip():
            return {
                'pattern': 'ë¶„ì„ë¶ˆê°€',
                'confidence': 0.0,
                'confidence_level': 'ì—†ìŒ',
                'description': 'ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.',
                'all_probabilities': {}
            }
        
        try:
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=128,
                return_tensors='pt'
            )
            
            # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì…ë ¥ í…ì„œë¥¼ GPUë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ëª¨ë¸ ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
            
            # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  ê³„ì‚°
            probabilities = F.softmax(logits, dim=-1)
            probabilities = probabilities.cpu().numpy()[0]  # CPUë¡œ ì´ë™ í›„ numpy ë°°ì—´ë¡œ ë³€í™˜
            
            # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì°¾ê¸°
            predicted_class_id = np.argmax(probabilities)
            predicted_pattern = self.id_to_label[predicted_class_id]
            confidence = float(probabilities[predicted_class_id])
            
            # ëª¨ë“  í´ë˜ìŠ¤ë³„ í™•ë¥  ë”•ì…”ë„ˆë¦¬ ìƒì„±
            all_probabilities = {
                self.id_to_label[i]: float(prob) 
                for i, prob in enumerate(probabilities)
            }
            
            # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            result = {
                'pattern': predicted_pattern,
                'confidence': round(confidence, 3),
                'confidence_level': self._get_confidence_level(confidence),
                'description': self._get_pattern_description(predicted_pattern),
                'all_probabilities': all_probabilities
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                'pattern': 'ì˜¤ë¥˜',
                'confidence': 0.0,
                'confidence_level': 'ì—†ìŒ',
                'description': f'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
                'all_probabilities': {}
            }
    
    def predict_batch(self, texts: list) -> list:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì˜ˆì¸¡
        
        Args:
            texts (list): ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: ê° í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        for text in texts:
            result = self.predict(text)
            results.append(result)
        return results
    
    def get_model_info(self) -> Dict:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict: ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        return {
            'model_path': self.model_path,
            'device': str(self.device),
            'num_labels': self.num_labels,
            'label_to_id': self.label_to_id,
            'id_to_label': self.id_to_label,
            'available_patterns': list(self.pattern_descriptions.keys())
        }