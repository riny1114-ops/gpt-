#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KB AI Challenge - íˆ¬ì ì‹¬ë¦¬ ë¶„ì„ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ (ê²½ë¡œ ìˆ˜ì • ë²„ì „)
KB Reflex: AI íˆ¬ì ì‹¬ë¦¬ ì½”ì¹­ í”Œë«í¼
"""

import os
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report

# ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •
torch.manual_seed(42)
np.random.seed(42)

class InvestmentEmotionDataset(Dataset):
    """íˆ¬ì ë©”ëª¨ì™€ ê°ì • íƒœê·¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” PyTorch ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self) -> int:
        return len(self.texts)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def find_csv_files():
    """CSV íŒŒì¼ ìœ„ì¹˜ ìë™ íƒì§€"""
    possible_locations = [
        # í˜„ì¬ ë””ë ‰í† ë¦¬
        ["kim_gukmin_trades.csv", "park_tuja_trades.csv"],
        # data í´ë”
        ["data/kim_gukmin_trades.csv", "data/park_tuja_trades.csv"],
        # ì ˆëŒ€ ê²½ë¡œë¡œ data í´ë” í™•ì¸
        [str(Path("data/kim_gukmin_trades.csv").resolve()), 
         str(Path("data/park_tuja_trades.csv").resolve())],
    ]
    
    for file_list in possible_locations:
        if all(os.path.exists(file) for file in file_list):
            print(f"âœ… CSV íŒŒì¼ ë°œê²¬: {os.path.dirname(file_list[0]) or 'í˜„ì¬ ë””ë ‰í† ë¦¬'}")
            return file_list
    
    return None

def create_data_if_missing():
    """ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±"""
    print("ğŸ“Š CSV íŒŒì¼ì´ ì—†ì–´ì„œ ìë™ ìƒì„±í•©ë‹ˆë‹¤...")
    
    try:
        # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
        current_file = Path(__file__)
        project_root = current_file.parent
        
        # sys.pathì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
        import sys
        sys.path.insert(0, str(project_root))
        
        from db.user_db import UserDatabase
        db = UserDatabase()
        
        # ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
        data_dir = project_root / "data"
        return [
            str(data_dir / "kim_gukmin_trades.csv"),
            str(data_dir / "park_tuja_trades.csv")
        ]
        
    except Exception as e:
        print(f"âŒ ìë™ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ 'python db/user_db.py'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return None

def load_and_combine_data(file_paths: List[str]) -> pd.DataFrame:
    """ì—¬ëŸ¬ CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í†µí•©"""
    print("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
    dataframes = []
    
    for file_path in file_paths:
        try:
            # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
            encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is not None:
                print(f"âœ… {file_path} ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
                dataframes.append(df)
            else:
                print(f"âŒ {file_path} ì¸ì½”ë”© ì˜¤ë¥˜")
                
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            continue
        except Exception as e:
            print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            continue
    
    if not dataframes:
        raise ValueError("ë¡œë“œí•  ìˆ˜ ìˆëŠ” ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    combined_df = pd.concat(dataframes, ignore_index=True)
    print(f"ğŸ”„ ë°ì´í„° í†µí•© ì™„ë£Œ: ì´ {len(combined_df)}ê°œ ë ˆì½”ë“œ")
    
    return combined_df

def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, int], Dict[int, str]]:
    """ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¼ë²¨ ì¸ì½”ë”©"""
    print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_columns = ['ë©”ëª¨', 'ê°ì •íƒœê·¸']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
        print(f"ğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
    
    # ê²°ì¸¡ê°’ ì œê±°
    initial_len = len(df)
    df = df.dropna(subset=['ë©”ëª¨', 'ê°ì •íƒœê·¸'])
    print(f"ğŸ“ ê²°ì¸¡ê°’ ì œê±°: {initial_len} -> {len(df)}ê°œ ë ˆì½”ë“œ")
    
    # ë¹ˆ ë¬¸ìì—´ ì œê±°
    df = df[df['ë©”ëª¨'].str.strip() != '']
    df = df[df['ê°ì •íƒœê·¸'].str.strip() != '']
    print(f"ğŸ“ ë¹ˆ ê°’ ì œê±° í›„: {len(df)}ê°œ ë ˆì½”ë“œ")
    
    # ê°ì •íƒœê·¸ì—ì„œ # ê¸°í˜¸ ì œê±° (ìˆëŠ” ê²½ìš°)
    df['ê°ì •íƒœê·¸'] = df['ê°ì •íƒœê·¸'].str.replace('#', '', regex=False)
    
    # ë¼ë²¨ ì¸ì½”ë”©
    label_encoder = LabelEncoder()
    df['label_encoded'] = label_encoder.fit_transform(df['ê°ì •íƒœê·¸'])
    
    # ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    label_to_id = {label: idx for idx, label in enumerate(label_encoder.classes_)}
    id_to_label = {idx: label for label, idx in label_to_id.items()}
    
    print(f"ğŸ·ï¸  ê°ì • ë¼ë²¨ ë§¤í•‘:")
    for label, idx in label_to_id.items():
        count = len(df[df['ê°ì •íƒœê·¸'] == label])
        print(f"   {idx}: {label} ({count}ê°œ)")
    
    return df, label_to_id, id_to_label

def create_train_val_split(df: pd.DataFrame, test_size: float = 0.2, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì…‹ ë¶„í• """
    print("âœ‚ï¸  ë°ì´í„° ë¶„í•  ì¤‘...")
    
    train_df, val_df = train_test_split(
        df, 
        test_size=test_size, 
        random_state=random_state,
        stratify=df['label_encoded']
    )
    
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(train_df)}ê°œ")
    print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(val_df)}ê°œ")
    
    return train_df, val_df

def initialize_model_and_tokenizer(model_name: str, num_labels: int) -> Tuple[Any, Any]:
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""
    print(f"ğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels
    )
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë¼ë²¨ ìˆ˜: {num_labels})")
    return model, tokenizer

def compute_metrics(eval_pred) -> Dict[str, float]:
    """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(labels, predictions)
    
    return {'accuracy': accuracy}

def save_model_info(output_dir: str, label_to_id: Dict[str, int], id_to_label: Dict[int, str]):
    """ëª¨ë¸ ì •ë³´ ë° ë¼ë²¨ ë§¤í•‘ ì €ì¥"""
    model_info = {
        'label_to_id': label_to_id,
        'id_to_label': {str(k): v for k, v in id_to_label.items()},
        'num_labels': len(label_to_id),
        'model_type': 'BERT-based emotion classifier for investment psychology'
    }
    
    info_path = os.path.join(output_dir, 'model_info.json')
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ëª¨ë¸ ì •ë³´ ì €ì¥: {info_path}")

def main():
    """ë©”ì¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸"""
    print("ğŸš€ KB Reflex íˆ¬ì ì‹¬ë¦¬ ë¶„ì„ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    print("=" * 60)
    
    # ì„¤ì •ê°’
    MODEL_NAME = "klue/bert-base"
    OUTPUT_DIR = "./sentiment_model"
    
    try:
        # 1. CSV íŒŒì¼ ì°¾ê¸° ë˜ëŠ” ìƒì„±
        CSV_FILES = find_csv_files()
        
        if CSV_FILES is None:
            CSV_FILES = create_data_if_missing()
            if CSV_FILES is None:
                raise FileNotFoundError("CSV íŒŒì¼ì„ ì°¾ê±°ë‚˜ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # 2. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
        df = load_and_combine_data(CSV_FILES)
        df, label_to_id, id_to_label = preprocess_data(df)
        
        # 3. ë°ì´í„° ë¶„í• 
        train_df, val_df = create_train_val_split(df)
        
        # 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        model, tokenizer = initialize_model_and_tokenizer(MODEL_NAME, len(label_to_id))
        
        # 5. ë°ì´í„°ì…‹ ìƒì„±
        print("ğŸ¯ PyTorch ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        train_dataset = InvestmentEmotionDataset(
            texts=train_df['ë©”ëª¨'].tolist(),
            labels=train_df['label_encoded'].tolist(),
            tokenizer=tokenizer
        )
        
        val_dataset = InvestmentEmotionDataset(
            texts=val_df['ë©”ëª¨'].tolist(),
            labels=val_df['label_encoded'].tolist(),
            tokenizer=tokenizer
        )
        
        # 6. í›ˆë ¨ ì„¤ì • (ë²„ì „ í˜¸í™˜ì„±)
        try:
            training_args = TrainingArguments(
                output_dir=OUTPUT_DIR,
                num_train_epochs=3,
                per_device_train_batch_size=16,
                per_device_eval_batch_size=64,
                warmup_steps=100,
                weight_decay=0.01,
                eval_strategy="steps",
                save_strategy="steps",
                eval_steps=50,
                save_steps=50,
                logging_steps=50,
                load_best_model_at_end=True,
                metric_for_best_model="accuracy",
                greater_is_better=True,
                save_total_limit=2,
                remove_unused_columns=False,
            )
        except TypeError:
            # êµ¬ ë²„ì „ í˜¸í™˜ì„±
            print("âš ï¸  êµ¬ ë²„ì „ transformers ê°ì§€, í˜¸í™˜ì„± ëª¨ë“œ...")
            training_args = TrainingArguments(
                output_dir=OUTPUT_DIR,
                num_train_epochs=3,
                per_device_train_batch_size=16,
                per_device_eval_batch_size=64,
                warmup_steps=100,
                weight_decay=0.01,
                evaluation_strategy="steps",
                save_strategy="steps",
                eval_steps=50,
                save_steps=50,
                logging_steps=50,
                load_best_model_at_end=True,
                metric_for_best_model="accuracy",
                greater_is_better=True,
                save_total_limit=2,
                remove_unused_columns=False,
            )
        
        # 7. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        print("ğŸƒâ€â™‚ï¸ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì¤‘...")
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=tokenizer,
            compute_metrics=compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
        )
        
        # 8. ëª¨ë¸ í›ˆë ¨
        print("ğŸ“ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        print("=" * 60)
        trainer.train()
        
        # 9. ìµœì¢… í‰ê°€
        print("ğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...")
        eval_results = trainer.evaluate()
        print(f"âœ… ìµœì¢… ê²€ì¦ ì •í™•ë„: {eval_results['eval_accuracy']:.4f}")
        
        # 10. ëª¨ë¸ ì €ì¥
        print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        trainer.save_model()
        tokenizer.save_pretrained(OUTPUT_DIR)
        
        # 11. ëª¨ë¸ ì •ë³´ ì €ì¥
        save_model_info(OUTPUT_DIR, label_to_id, id_to_label)
        
        # 12. ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±
        print("ğŸ“‹ ë¶„ë¥˜ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        predictions = trainer.predict(val_dataset)
        y_pred = np.argmax(predictions.predictions, axis=1)
        y_true = val_df['label_encoded'].tolist()
        
        print("\nğŸ“Š ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:")
        print("=" * 60)
        target_names = [id_to_label[i] for i in range(len(id_to_label))]
        print(classification_report(y_true, y_pred, target_names=target_names))
        
        print("\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
        print("=" * 60)
        
        # 13. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸:")
        test_texts = [
            "ì½”ìŠ¤í”¼ê°€ ë„ˆë¬´ ë–¨ì–´ì ¸ì„œ ë¬´ì„œì›Œì„œ ì „ëŸ‰ ë§¤ë„í–ˆì–´ìš”",
            "ìœ íŠœë²„ ì¶”ì²œë°›ê³  ê¸‰í•˜ê²Œ ë§¤ìˆ˜í–ˆìŠµë‹ˆë‹¤",
            "ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼ ì ì • ë§¤ìˆ˜ íƒ€ì´ë°ìœ¼ë¡œ íŒë‹¨ë¨"
        ]
        
        for test_text in test_texts:
            inputs = tokenizer(test_text, return_tensors="pt", truncation=True, max_length=128)
            with torch.no_grad():
                outputs = model(**inputs)
                predicted_id = torch.argmax(outputs.logits, dim=-1).item()
                predicted_emotion = id_to_label[predicted_id]
                confidence = torch.softmax(outputs.logits, dim=-1).max().item()
            
            print(f"ğŸ“ '{test_text[:30]}...'")
            print(f"ğŸ¯ ì˜ˆì¸¡: {predicted_emotion} (ì‹ ë¢°ë„: {confidence:.3f})")
            print()
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()